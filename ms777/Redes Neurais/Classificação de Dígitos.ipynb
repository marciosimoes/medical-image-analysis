{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    # Método do Gradiente\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            if test_data:\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
    "            else:\n",
    "                print(\"Epoch {} finalizada\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    # Algoritmo do Backpropagation    \n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # Feedforward\n",
    "        activation = x\n",
    "\n",
    "        # Lista para armazenar todas as ativações, camada por camada\n",
    "        activations = [x] \n",
    "\n",
    "        # Lista para armazenar todos os vetores z, camada por camada\n",
    "        zs = [] \n",
    "\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # Backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        # Aqui, l = 1 significa a última camada de neurônios, l = 2 é a segunda e assim por diante. \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.27067934,  0.24600742,  0.25309133,  0.31032033],\n",
       "        [-1.41098674,  1.09301612,  0.56679762, -0.28497747]]),\n",
       " array([[-0.27166406,  0.97635304],\n",
       "        [-1.16488214,  1.56297069],\n",
       "        [-0.08747132,  0.18263885]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testando\n",
    "rede1 = Network([4,2,3])\n",
    "rede1.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de Ativação Sigmóide\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "# Função para retornar as derivadas da função Sigmóide\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções de manipulação dos dados\n",
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação dos dados: treinamento e teste\n",
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "training_data = list(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 9110 / 10000\n",
      "Epoch 1 : 9261 / 10000\n",
      "Epoch 2 : 9313 / 10000\n",
      "Epoch 3 : 9371 / 10000\n",
      "Epoch 4 : 9396 / 10000\n",
      "Epoch 5 : 9399 / 10000\n",
      "Epoch 6 : 9444 / 10000\n",
      "Epoch 7 : 9447 / 10000\n",
      "Epoch 8 : 9469 / 10000\n",
      "Epoch 9 : 9493 / 10000\n",
      "Epoch 10 : 9493 / 10000\n",
      "Epoch 11 : 9493 / 10000\n",
      "Epoch 12 : 9507 / 10000\n",
      "Epoch 13 : 9480 / 10000\n",
      "Epoch 14 : 9472 / 10000\n",
      "Epoch 15 : 9498 / 10000\n",
      "Epoch 16 : 9463 / 10000\n",
      "Epoch 17 : 9505 / 10000\n",
      "Epoch 18 : 9474 / 10000\n",
      "Epoch 19 : 9506 / 10000\n",
      "Epoch 20 : 9473 / 10000\n",
      "Epoch 21 : 9492 / 10000\n",
      "Epoch 22 : 9503 / 10000\n",
      "Epoch 23 : 9495 / 10000\n",
      "Epoch 24 : 9508 / 10000\n",
      "Epoch 25 : 9503 / 10000\n",
      "Epoch 26 : 9504 / 10000\n",
      "Epoch 27 : 9509 / 10000\n",
      "Epoch 28 : 9502 / 10000\n",
      "Epoch 29 : 9486 / 10000\n"
     ]
    }
   ],
   "source": [
    "# Criação e treinamento da rede\n",
    "net = Network([784, 30, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f , encoding=\"latin1\")\n",
    "    \n",
    "valid_x, valid_y = valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#um teste aleatório\n",
    "image = valid_x[randint(0,10000)]\n",
    "\n",
    "plt.imshow(image.reshape((28, 28)), cmap=cm.Greys_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidade de ser 0 = 0.0%\n",
      "Probabilidade de ser 1 = 11.2%\n",
      "Probabilidade de ser 2 = 85.33%\n",
      "Probabilidade de ser 3 = 0.0%\n",
      "Probabilidade de ser 4 = 0.1%\n",
      "Probabilidade de ser 5 = 0.0%\n",
      "Probabilidade de ser 6 = 3.37%\n",
      "Probabilidade de ser 7 = 0.0%\n",
      "Probabilidade de ser 8 = 0.0%\n",
      "Probabilidade de ser 9 = 0.0%\n"
     ]
    }
   ],
   "source": [
    "#resultado\n",
    "layer1 = sigmoid(np.dot(net.weights[0], image))\n",
    "layer2 = sigmoid(np.dot(net.weights[1], layer1))\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(\"Probabilidade de ser {} = {}%\".format(i,round((layer2[i]*100)/sum(layer2),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
